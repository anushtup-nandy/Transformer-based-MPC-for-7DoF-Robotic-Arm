# Transformer-based Model Predictive Control for KUKA LBR4

Implementation of **temporal-aware data-driven MPC** using Transformer networks for precise robotic trajectory tracking, building on the work by El-Hussieny et al. (2024).

## ğŸ“‹ Overview

This project implements and compares two neural architectures for predictive modeling in MPC:
- **Baseline Feed-Forward DNN** (El-Hussieny et al., 2024)
- **Transformer with Multi-Head Self-Attention** (Proposed)

The Transformer architecture captures temporal dependencies in robot motion, leading to improved prediction accuracy and control performance.

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Create conda environment
conda env create -f environment.yml
conda activate transformer-mpc-kuka

# Verify installation
python -c "import torch; import casadi; print('âœ“ Installation successful')"
```

### 2. Generate Synthetic Data

```bash
python src/data_generator.py
```

This generates ~20,000 samples with:
- Pick-and-place trajectories
- Circular motions
- Figure-8 patterns
- Sinusoidal movements
- Realistic sensor noise

### 3. Train Models

Train the baseline DNN:
```bash
python src/train.py --model_type baseline
```

Train the Transformer:
```bash
python src/train.py --model_type transformer
```

Monitor training with TensorBoard:
```bash
tensorboard --logdir logs
```

### 4. Evaluate Performance

```bash
python src/evaluate.py
```

This will:
- Run both controllers on test trajectories
- Generate comparison plots
- Compute performance metrics
- Save results to `figures/`

## ğŸ“ Project Structure

```
transformer-mpc-kuka/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml              # Hyperparameters & settings
â”œâ”€â”€ data/
â”‚   â””â”€â”€ synthetic_dataset.npz    # Generated training data
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ baseline_dnn.py          # Feed-forward DNN
â”‚   â”œâ”€â”€ transformer_predictor.py # Transformer architecture
â”‚   â””â”€â”€ trained/                 # Saved checkpoints
â”‚       â”œâ”€â”€ baseline/
â”‚       â””â”€â”€ transformer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_generator.py        # PyBullet-based data generation
â”‚   â”œâ”€â”€ train.py                 # Training script
â”‚   â”œâ”€â”€ mpc_controller.py        # CasADi MPC implementation
â”‚   â””â”€â”€ evaluate.py              # Evaluation & visualization
â”œâ”€â”€ kuka_iiwa/                   # URDF files
â”œâ”€â”€ figures/                     # Generated plots
â”œâ”€â”€ logs/                        # TensorBoard logs
â”œâ”€â”€ environment.yml              # Conda environment
â””â”€â”€ README.md
```

## ğŸ—ï¸ Architecture Details

### Transformer Model

```
Input Embedding (21 â†’ 128)
    â†“
Positional Encoding
    â†“
Transformer Encoder (3 layers)
  â€¢ Multi-Head Attention (4 heads)
  â€¢ Feed-Forward Network (256 dim)
  â€¢ Layer Normalization
  â€¢ Dropout (0.1)
    â†“
Output Projection (128 â†’ 14)
```

**Key Features:**
- History length: 10 timesteps
- Self-attention captures temporal patterns
- Residual connections for training stability
- ~165K parameters

### Baseline DNN

```
Input (21) â†’ [128] â†’ ReLU â†’ [32] â†’ ReLU â†’ Output (14)
```

**Key Features:**
- Memoryless (single timestep)
- Simple feed-forward architecture
- ~4K parameters

## ğŸ¯ MPC Formulation

The MPC optimization problem:

```
minimize:  Î£(||q - q_ref||Â²_W1 + ||Î”Ï„||Â²_W2)

subject to:
  â€¢ x(k+1) = f_learned(x(k), Ï„(k))   [Neural network dynamics]
  â€¢ q_min â‰¤ q(k) â‰¤ q_max              [Joint limits]
  â€¢ Ï„_min â‰¤ Ï„(k) â‰¤ Ï„_max              [Torque limits]
  â€¢ dq_min â‰¤ dq(k) â‰¤ dq_max           [Velocity limits]
```

**Solver:** IPOPT via CasADi  
**Prediction Horizon:** N = 12  
**Sampling Time:** 50ms

## ğŸ“Š Expected Results

Based on the proposal, the Transformer model should achieve:

- **30-50% reduction** in prediction MSE
- **Improved tracking** on complex trajectories
- **Better generalization** to unseen patterns
- **Real-time capable** (< 50ms per control step)

## ğŸ”§ Configuration

Edit `configs/config.yaml` to modify:

**Model Architecture:**
```yaml
transformer:
  history_length: 10
  d_model: 128
  num_heads: 4
  num_encoder_layers: 3
```

**Training:**
```yaml
training:
  batch_size: 64
  num_epochs: 100
  learning_rate: 0.001
```

**MPC:**
```yaml
mpc:
  prediction_horizon: 12
  weights:
    state: 100.0
    control: 0.01
```

## ğŸ“ˆ Monitoring Training

TensorBoard provides real-time monitoring:

```bash
tensorboard --logdir logs
```

View at: `http://localhost:6006`

Metrics tracked:
- Training/validation loss
- Learning rate schedule
- Gradient norms
- Prediction accuracy

## ğŸ§ª Testing Different Scenarios

The evaluation script tests multiple scenarios:

1. **Point Stabilization**: Step changes in target positions
2. **Circular Trajectory**: Continuous circular motion in joint space
3. **Figure-8**: Complex curved path with direction changes
4. **Sinusoidal**: Multi-frequency oscillatory motion

## ğŸ› Troubleshooting

### URDF Not Found
```bash
# Ensure URDF is in correct location:
ls kuka_iiwa/urdf/iiwa7.urdf
```

### IPOPT Solver Fails
```bash
# Reduce prediction horizon in config.yaml:
mpc:
  prediction_horizon: 8  # Try smaller value
```

### Out of Memory During Training
```bash
# Reduce batch size:
training:
  batch_size: 32  # Or 16
```

### Slow Training on CPU
```bash
# Check CUDA availability:
python -c "import torch; print(torch.cuda.is_available())"
```

## ğŸ“ Key Implementation Notes

1. **Temporal Context**: Transformer uses sliding window of past 10 timesteps
2. **Positional Encoding**: Sinusoidal encoding preserves temporal order
3. **Noise Injection**: Gaussian noise added to simulate real sensors
4. **Constraint Handling**: MPC enforces physical limits via CasADi
5. **Real-time Feasibility**: Lightweight architecture for fast inference

## ğŸ”¬ Research Extensions

Potential improvements:
- [ ] LSTM comparison (mentioned in proposal)
- [ ] Attention visualization
- [ ] Longer history windows
- [ ] Hardware deployment
- [ ] Disturbance rejection tests
- [ ] Multi-robot coordination

## ğŸ“š References

1. El-Hussieny et al. (2024). "Advancing Robotic Control: Data-Driven Model Predictive Control for a 7-DOF Robotic Manipulator." *IEEE Access*.

2. Vaswani et al. (2017). "Attention Is All You Need." *NeurIPS*.

3. Andersson et al. (2019). "CasADi: A Software Framework for Nonlinear Optimization and Optimal Control." *Mathematical Programming Computation*.

## ğŸ‘¥ Authors

Anushtup Nandy  
Department of Mechanical Engineering  
Columbia University

